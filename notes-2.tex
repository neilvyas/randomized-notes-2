\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\DeclareMathOperator*{\var}{\mathbf{Var}}

\newcommand{\eps}{\epsilon}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\R}{\mathbb{R}}

%complexity theory variables.
\newcommand{\Pt}{\ensuremath{\mathbf{P}}}
\newcommand{\NP}{\ensuremath{\mathbf{NP}}}
\newcommand{\PP}{\ensuremath{\mathbf{PP}}}
\newcommand{\BPP}{\ensuremath{\mathbf{BPP}}}
\newcommand{\ZPP}{\ensuremath{\mathbf{Z_{PP}}}}
\newcommand{\RP}{\ensuremath{\mathbf{R_{P}}}}
\newcommand{\coRP}{\ensuremath{\mathbf{coR_{P}}}}
\newcommand{\Ppoly}{\ensuremath{\mathbf{P_{poly}}}}

\newcommand{\Alg}{\ensuremath{\mathcal{A}}}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf CS 388R: Randomized Algorithms } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe: #4}{Lecture #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex

%for wrapping inline math mode.
\let\ab\allowbreak

\begin{document}

\lecture{2 --- 31 August 2014}{Fall 2015}{Prof.\ Eric Price}{Sid Kapur, Neil Vyas}

\section{Overview}

In the last lecture we \ldots.

In this lecture we \ldots.

\section{Food for Thought}

\begin{enumerate}
  \item Suppose you flip an unbiased coin 1000 times. How surprised would you be if we observed
    \begin{itemize}
      \item 500 heads?
      \item 510 heads?
      \item 600 heads?
      \item 1000 heads?
    \end{itemize}
  \item Suppose you have a biased coin that lands heads with an unknown probability $p$. How many flips will it take to learn $p$ to an error of $\pm \eps$ with probability $1 - \delta$?
\end{enumerate}

\section{Probability Background}
\begin{theorem}
  \textbf{Linearity of expectation.}

  If $X_1,\dots,X_n$ are random variables, then
  $$ \E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \E[X_i] $$
\end{theorem}

Example: Suppose $X_1,\dots,X_n$ are Bernoulli trials with $p=0.5$. Let $X = \sum_{i=1}^n E_i$. Then
$$ \E[X] = \sum_{i=1}^n \E[X_i] = \frac{n}{2} $$

\begin{definition}
  The variance of a random variable $X$, denoted $\var[X]$, is defined as
  $$ \var[X] = \E\left[(X - \E[X])^2\right] $$
\end{definition}

\begin{observation}
Note that if $Y_1$ and $Y_2$ are independent random variables with $\E[Y_1] = \E[Y_2] = 0$, then
\begin{align*}
  \var[Y_1 + Y_2] &= \E\left[(Y_1 + Y_2)^2\right] \\
                  &= \E\left[Y_1^2 + Y_2^2 + 2Y_1Y_2\right] \\
                  &= \E\left[Y_1^2\right] + \E\left[Y_2^2\right] + 2\E[Y_1]\E[Y_2] &\small\text{(since $Y_1$ and $Y_2$ are independent)}\\
                  &= \E\left[Y_1^2\right] + \E\left[Y_2^2\right] \\
                  &= \var[Y_1] + \var[Y_2]
\end{align*}
\end{observation}

\begin{claim}\label{claima}
We claim (without proof) that if $Y_1$ and $Y_2$ are independent, then $\var[Y_1 + Y_2] = \var[Y_1] + \var[Y_2]$ (i.e. $\E[Y_1]$ and $\E[Y_2]$ need not be zero).
\end{claim}

Example: Suppose $X_1,\dots,X_n$ are independent Bernoulli trials with $p=0.5$. Let $X = \sum_{i=1}^n E_i$. Then
\begin{align*}
  \var[X] &= \sum_{i=1}^n \var[X_i] &\small\text{(by Claim \ref{claima})}\\
          &= n\left[ \frac12 \left(\frac12\right)^2 + \frac12 \left(\frac12\right)^2 \right] \\
          &= \frac{n}{4}
\end{align*}

If $n=1000$, then $\var[X] = 250$ and the standard deviation of $X = \sqrt{\var[X]} \approx 16$.

\begin{theorem}
  \textbf{The central limit theorem}
  Suppose $X_1,\dots,X_n$ are independent identically distributed random variables with expected value $\mu$ and variance $\sigma^2$.

  Then as $n \to \infty$, the sample average $S_n = \frac{X_1 + \dots + X_n}{n}$ converges to the Gaussian
  $$ N\left(\mu, \frac{\sigma^2}{n}\right) $$

  Some statements of the central limit theorem give bounds on convergence, but unfortunately these bounds are not tight enough to be useful in this example.
\end{theorem}

By the central limit theorem, as the number of coin tosses goes to infinity, our binomial distribution of coin tosses converges to a Gaussian $N(\mu, \sigma^2)$ where $\mu = \frac{n}{2}$ and $\sigma^2 = \frac{n}{4}$, as we calculated above.

\section{Bounding the Sum of Bernoulli Trials}

Let's return to our coin-tossing problem.

Can we find an upper bound on the probability that the number of coin tosses exceeds a certain number?

\begin{claim}\label{gaussianbound}
  If $Z$ is drawn from a Gaussian with mean $\mu$ and variance $\sigma^2$, then
  \begin{align*}
    \Pr[Z \ge \mu + t] &\le e^{-\frac{t^2}{2\sigma^2}} \quad \forall t \ge 0 \\
    \Pr[Z \le \mu - t] &\le e^{-\frac{t^2}{2\sigma^2}} \quad \forall t \ge 0
  \end{align*}
  (We omit the proof.)
\end{claim}

If our distribution were Gaussian (or if we were willing to invoke the central limit theorem), we could use the bound from Claim \ref{gaussianbound}:
\begin{align*}
  &&\Pr[X \ge 500 + \sigma t] &\le e^{-\frac{t^2}{2}} \\
  &\Rightarrow &\Pr[X \ge 500 + 16t] &\le e^{-\frac{t^2}{2}} \\
  &\Rightarrow &\Pr[X \ge 600] &\le e^{-18} \\
\end{align*}
But unfortunately, we have a binomial distribution, not a Gaussian, and we don't want to use the central limit theorem. What should we do?

\begin{definition}
  \textbf{Chernoff bound (for the discrete setting)}
  Let $X = \sum_{i=1}^n X_i$, where the $X_i$ take on values in $[0,1]$ and are independent. (The $X_i$ can take on any value between 0 and 1. Also, the $X_i$ need not be identical.)
  Then
  \begin{align*}
    \Pr[X \ge \E[X] + t] &\le e^{-\frac{2t^2}{n}} \\
    \Pr[X \le \E[X] - t] &\le e^{-\frac{2t^2}{n}}
  \end{align*}
\end{definition}

If we use the Chernoff bound, we get:
\begin{align*}
  \Pr[X \ge 500 + \sigma t]
    &\le e^{-\frac{2t^2\sigma^2}{n}} \\
    &= e^{-\frac{2t^2(n/4)}{n}} &\small\text{(since $\sigma^2 = \frac{n}{4}$)}\\
    &= e^{-\frac{t^2}{2}}
\end{align*}

% TODO Did he say that the Chernoff bound is basically tight,
% or that the Gaussian bound is basically tight?
Note that this is the same bound that we got from the Gaussian! So the Chernoff bound is basically tight in this case.

\subsection{Evaluating the Chernoff Bound}
There are a few limitations of the Chernoff bound in this situation.

The main limitation is that it does not depend on the variance of the $X_i$. Suppose each $X_i$ has a very low probability, say,
$$\Pr[X_i] = \frac{1}{10\sqrt{n}}$$
Then
$$ \E[X] = \frac{1}{10\sqrt{n}} \cdot n = \frac{\sqrt{n}}{10} $$
Then the Chernoff bound would give us $1 - \delta$ confidence interval of
\begin{align*}
  && \Pr[X \ge \E[X] + t] \le e^{-\frac{2t^2}{n}} = \frac{\delta}{2} \\
  &\Rightarrow &t = \sqrt{\frac{n\log(2/\delta)}{2}}
\end{align*}
So the Chernoff bound gives us $X \in \E[X] \pm \sqrt{\frac{n\log(2/\delta)}{2}}$ with probability $1 - \delta$.

If we pick $\delta = 0.10$, then the bound is $X \in \E[X] \pm \sqrt{10n}$. Note that the width of our confidence interval is about 60 times the expected value!

We know that the bound can be made much tighter here because the variance of the $X_i$ is small. The Chernoff bound doesn't take variance into account. Later in this class, we will encounter Bernstein-type inequalities, which do account for the variance.

Also, note that the Chernoff bound can be less than 0 or greater than $n$, which does not make sense in this problem.

\subsection{Second Food for Thought}
\textbf{Suppose you have a biased coin that lands heads with an unknown probability $p$. How many flips will it take to learn $p$ to an error of $\pm \eps$ with probability $1 - \delta$?}

We need to find $n$ such that
$$ \Pr[X \ge (p + \eps)n] \le \frac{\delta}{2} $$
By the Chernoff bound,
\begin{align*}
  \Pr[X \ge (p + \eps)n] &\le e^{-\frac{2(\eps n)^2}{n}} \\
                         &= e^{-2\eps^2 n}
\end{align*}
So we must pick $n$ such that
$$ e^{-2\eps^2 n} \le \frac \delta 2 $$
i.e.
$$ n \ge \frac{1}{2\eps^2}\log\left(\frac{2}{\delta} \right) $$

\section{The Coupon Collecting Problem}

\textbf{Suppose there exist $n$ types of coupons, with an infinite supply of each. If you collect coupons uniformly at random, what is the expected number of coupons you must collect to hold at least one of each type?}

Let's consider the case in which we hold $i$ types of coupons already and determine how many we expect to collect before obtaining a new type; denote this expectation as $T_i$.

Then
\begin{align*}
  T_0 &= 1 \\
  T_1 &= 2 \\
  \ldots \\
  T_{n-1} &= n \\
  \ldots \\
  T_{n-k} &= n / k \\
  \Rightarrow T = \sum{T_i} &= n \sum_{i=1}^n \frac{1}{k} \leq n(\log n + 1)
\end{align*}
Note that $T_{n-k} = n / k$ precisely because we draw coupons uniformly at random.

\section{Background on Complexity Theory}

Let's begin our whirlwind tour of complexity theory. We'll explore, to varying degrees, \Pt,\ \NP,\ab\ \ZPP,\ \RP,\ \coRP, and \BPP.

We use as supplemental material section 1.5 from \textit{Randomized Algorithms} by Motwani and Raghavan.

\subsection{\Pt: Polynomial-time deterministic algorithms}

These can be described by Turing machines and $languages$. A $language$ $\ell \subset \{0,1\}^n$ such that for a candidate $x,\ x \in \ell \Rightarrow$ we accept $x$ as an answer; otherwise we reject.
Therefore, given an interactive protocol for checking against $\ell$, we can use binary search to determine $x$.

Formally, the class \Pt\ consists of all languages $\ell$ that have a polynomial-time algorithm \Alg\ s.t. for any input $x$, 
\begin{align*}
  x \in \ell &\Rightarrow \Alg \text{ accepts} \\
  x \notin \ell &\Rightarrow \Alg \text{ accepts}
\end{align*}

\subsection{\NP: \Pt\ with a good ``advice'' string}

``Advice'' given to a Turing machine is an additional input string allowed to depend on the length of the input, $n$, but not the input itself. A problem is in $\NP \Leftrightarrow \forall\text{inputs, }\exists$ advice such that an algorithm with the given advice always decides correctly. If the advice string is random, then the algorithm is randomized.

Note that for our purposes, the advice string may be arbitrarily long, but the algorithm can only inspect a polynomial-in-the-length-of-the-input amount of it.

\subsection{\RP\ and \coRP: One-sided error }

Suppose that $\Alg\in\RP$. Then $\forall$ inputs $x$,
\begin{align*}
  x \in \ell &\Rightarrow \Pr[\Alg \text{ accepts}] \geq \frac{1}{2} \\
  x \notin \ell &\Rightarrow \Pr[\Alg \text{ accepts}] = 0
\end{align*}
and \Alg\ has polynomial worst-case runtime.

\coRP\ is similar, but instead of erring when $x\in\ell$, an algorithm in \coRP\ can err when $x\notin\ell$.

Note that an \RP\ algorithm corresponds to a Monte Carlo algorithm that can err only when $x \in \ell$, and similarly, an algorithm in \coRP\ corresponds to a Monte Carlo algorithm that can err only when $x \notin \ell$. 

\subsection{\ZPP\: Zero-error with expected polynomial time}

This complexity class corresponds to Las Vegas-type randomized algorithms. Thus, you might expect that $\ZPP = \RP \cap \coRP$.

\subsection{\PP\ and \BPP: Two-sided error }

\PP, or Probabalistic Polynomial time, consists of algorithms that run in polynomial worst-case time with the following additional constraint:

Suppose that $\Alg\in\PP$. Then $\forall$ inputs $x$,
\begin{align*}
  x \in \ell &\Rightarrow \Pr[\Alg \text{ accepts}] \ge \frac{1}{2} \\
  x \notin \ell &\Rightarrow \Pr[\Alg \text{ accepts}] \le \frac{1}{2} \\
\end{align*}
and \Alg\ has polynomial worst-case runtime.

However, the nature of this definition means that \PP\ is not a very useful class; for example, $\NP\subset\PP$. To consider this we consider SAT, the unrestricted satisfiability problem: Given $f(x_1, \dots, x_n)$, is it satisfiable?

We propose the following algorithm: %format this like an algo, maybe?

%TODO Sid can you format this please?
Choose $(x_1, \dots, x_n)$ uniformly at random. \\
 if $f(x_1, \dots, x_n) = 1$: \textbf{return} ``yes''\\
 else: \textbf{return} ``yes'' with $\Pr = \frac{1}{2}$

Suppose that the input is satisfiable; then $\Pr[\text{yes}] \geq \frac{1}{2} + \frac{1}{2^{n+1}}$.

Thus we can solve SAT in \PP.

Note that since we can reduce every problem in \NP\ to SAT, we've just shown that we can solve any problem in \NP\ in \PP. Thus, \PP\ isn't a very useful class to reason about.

We introduce \BPP\ in hopes oof remedying this situation; namely, suppose that $\Alg\in\BPP$. 

Then $\forall$ inputs $x$,
\begin{align*}
  x \in \ell &\Rightarrow \Pr[\Alg \text{ accepts}] \geq \frac{3}{4} \\
  x \notin \ell &\Rightarrow \Pr[\Alg \text{ accepts}] \leq \frac{1}{4} 
\end{align*}

Note that the specific numbers used for the bounds are able to be changed to 
$$\frac{1}{2} \pm \frac{1}{p(n)}$$
for any polynomial $p(n)$.

Additionally, we can show that the probability of error can be reduced to $\frac{1}{2^n}$ in a polynomial number of iterations; we'll return to this shortly.

\subsection{\Ppoly\ and amplification}

\subsection{Containments}

It is known that $\Pt\subset\ZPP$, but it is not known whether $\ZPP\subset\Pt$.
It should also be apparent that $\ZPP = \RP\cap\coRP$, as stated above.

We claim that $\Pt\subset\RP\subset\NP$, and that $\RP\subset\BPP$; here we will only prove that $\RP\subset\NP$.

$\mathbf{pf: }$
Let $x$ be a candidate solution to an \RP\ problem. 
\begin{align*}
  x\notin\ell &\Rightarrow \text{we discard the advice string}\\
  x\in\ell &\Rightarrow \exists\text{ an advice string $a$ that can prove that.}\\
  &\Rightarrow \RP \Rightarrow \NP \text{ involves fixing an advice string.} 
\end{align*}

It is not known whether the following statements hold:

$\BPP\subset\NP$

$\RP=\ZPP=\coRP$

\section{Adleman's Theorem}

\bibliographystyle{alpha}

\begin{thebibliography}{42}

% \bibitem[AMS99]{AMS99}
% Noga~Alon, Yossi~Matias, Mario~Szegedy.
% \newblock The Space Complexity of Approximating the Frequency Moments.
% \newblock {\em J. Comput. Syst. Sci.}, 58(1):137--147, 1999.

\end{thebibliography}

\end{document}
