\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\DeclareMathOperator*{\var}{\mathbf{Var}}

\newcommand{\eps}{\epsilon}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\R}{\mathbb{R}}

%complexity theory variables.
\newcommand{\Pt}{\ensuremath{\mathbf{P}}}
\newcommand{\NP}{\ensuremath{\mathbf{NP}}}
\newcommand{\PP}{\ensuremath{\mathbf{PP}}}
\newcommand{\BPP}{\ensuremath{\mathbf{BPP}}}
\newcommand{\ZPP}{\ensuremath{\mathbf{Z_{PP}}}}
\newcommand{\RP}{\ensuremath{\mathbf{R_{P}}}}
\newcommand{\coRP}{\ensuremath{\mathbf{coR_{P}}}}
\newcommand{\Ppoly}{\ensuremath{\mathbf{P_{poly}}}}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf CS 388R: Randomized Algorithms } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe: #4}{Lecture #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex

%for wrapping inline math mode.
\let\ab\allowbreak

\begin{document}

\lecture{2 --- 31 August 2014}{Fall 2015}{Prof.\ Eric Price}{Sid Kapur, Neil Vyas}

\section{Overview}

In the last lecture we \ldots.

In this lecture we \ldots.

\section{Food for Thought}

\begin{enumerate}
  \item Suppose you flip an unbiased coin 1000 times. How surprised would you be if we observed
    \begin{itemize}
      \item 500 heads?
      \item 510 heads?
      \item 600 heads?
      \item 1000 heads?
    \end{itemize}
  \item Suppose you have a biased coin that lands heads with an unknown probability $p$. How many flips will it take to learn $p$ to an error of $\pm \eps$ with probability $1 - \delta$?
\end{enumerate}

\section{Probability Background}
\begin{theorem}
  \textbf{Linearity of expectation.}

  If $X_1,\dots,X_n$ are random variables, then
  $$ \E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \E[X_i] $$
\end{theorem}

Example: Suppose $X_1,\dots,X_n$ are Bernoulli trials with $p=0.5$. Let $X = \sum_{i=1}^n E_i$. Then
$$ \E[X] = \sum_{i=1}^n \E[X_i] = \frac{n}{2} $$

\begin{definition}
  The variance of a random variable $X$, denoted $\var[X]$, is defined as
  $$ \var[X] = \E\left[(X - \E[X])^2\right] $$
\end{definition}

\begin{observation}
Note that if $Y_1$ and $Y_2$ are independent random variables with $\E[Y_1] = \E[Y_2] = 0$, then
\begin{align*}
  \var[Y_1 + Y_2] &= \E\left[(Y_1 + Y_2)^2\right] \\
                  &= \E\left[Y_1^2 + Y_2^2 + 2Y_1Y_2\right] \\
                  &= \E\left[Y_1^2\right] + \E\left[Y_2^2\right] + 2\E[Y_1]\E[Y_2] &\small\text{(since $Y_1$ and $Y_2$ are independent)}\\
                  &= \E\left[Y_1^2\right] + \E\left[Y_2^2\right] \\
                  &= \var[Y_1] + \var[Y_2]
\end{align*}
\end{observation}

\begin{claim}\label{claima}
We claim (without proof) that if $Y_1$ and $Y_2$ are independent, then $\var[Y_1 + Y_2] = \var[Y_1] + \var[Y_2]$ (i.e. $\E[Y_1]$ and $\E[Y_2]$ need not be zero).
\end{claim}

Example: Suppose $X_1,\dots,X_n$ are independent Bernoulli trials with $p=0.5$. Let $X = \sum_{i=1}^n E_i$. Then
\begin{align*}
  \var[X] &= \sum_{i=1}^n \var[X_i] &\small\text{(by Claim \ref{claima})}\\
          &= n\left[ \frac12 \left(\frac12\right)^2 + \frac12 \left(\frac12\right)^2 \right] \\
          &= \frac{n}{4}
\end{align*}

If $n=1000$, then $\var[X] = 250$ and the standard deviation of $X = \sqrt{\var[X]} \approx 16$.

\begin{theorem}
  \textbf{The central limit theorem}
  Suppose $X_1,\dots,X_n$ are independent identically distributed random variables with expected value $\mu$ and variance $\sigma^2$.

  Then as $n \to \infty$, the sample average $S_n = \frac{X_1 + \dots + X_n}{n}$ converges to the Gaussian
  $$ N\left(\mu, \frac{\sigma^2}{n}\right) $$

  Some statements of the central limit theorem give bounds on convergence, but unfortunately these bounds are not tight enough to be useful in this example.
\end{theorem}

By the central limit theorem, as the number of coin tosses goes to infinity, our binomial distribution of coin tosses converges to a Gaussian $N(\mu, \sigma^2)$ where $\mu = \frac{n}{2}$ and $\sigma^2 = \frac{n}{4}$, as we calculated above.

\section{Bounding the Sum of Bernoulli Trials}

Let's return to our coin-tossing problem.

Can we find an upper bound on the probability that the number of coin tosses exceeds a certain number?

\begin{claim}\label{gaussianbound}
  If $Z$ is drawn from a Gaussian with mean $\mu$ and variance $\sigma^2$, then
  \begin{align*}
    \Pr[Z \ge \mu + t] &\le e^{-\frac{t^2}{2\sigma^2}} \quad \forall t \ge 0 \\
    \Pr[Z \le \mu - t] &\le e^{-\frac{t^2}{2\sigma^2}} \quad \forall t \ge 0
  \end{align*}
  (We omit the proof.)
\end{claim}

If our distribution were Gaussian (or if we were willing to invoke the central limit theorem), we could use the bound from Claim \ref{gaussianbound}:
\begin{align*}
  &&\Pr[X \ge 500 + \sigma t] &\le e^{-\frac{t^2}{2}} \\
  &\Rightarrow &\Pr[X \ge 500 + 16t] &\le e^{-\frac{t^2}{2}} \\
  &\Rightarrow &\Pr[X \ge 600] &\le e^{-18} \\
\end{align*}
But unfortunately, we have a binomial distribution, not a Gaussian, and we don't want to use the central limit theorem. What should we do?

\begin{definition}
  \textbf{Chernoff bound (for the discrete setting)}
  Let $X = \sum_{i=1}^n X_i$, where the $X_i$ take on values in $[0,1]$ and are independent. (The $X_i$ can take on any value between 0 and 1. Also, the $X_i$ need not be identical.)
  Then
  \begin{align*}
    \Pr[X \ge \E[X] + t] &\le e^{-\frac{2t^2}{n}} \\
    \Pr[X \le \E[X] - t] &\le e^{-\frac{2t^2}{n}}
  \end{align*}
\end{definition}

If we use the Chernoff bound, we get:
\begin{align*}
  \Pr[X \ge 500 + \sigma t]
    &\le e^{-\frac{2t^2\sigma^2}{n}} \\
    &= e^{-\frac{2t^2(n/4)}{n}} &\small\text{(since $\sigma^2 = \frac{n}{4}$)}\\
    &= e^{-\frac{t^2}{2}}
\end{align*}

% TODO Did he say that the Chernoff bound is basically tight,
% or that the Gaussian bound is basically tight?
Note that this is the same bound that we got from the Gaussian! So the Chernoff bound is basically tight in this case.

\subsection{Evaluating the Chernoff Bound}
There are a few limitations of the Chernoff bound in this situation.

The main limitation is that it does not depend on the variance of the $X_i$. Suppose each $X_i$ has a very low probability, say,
$$\Pr[X_i] = \frac{1}{10\sqrt{n}}$$
Then
$$ \E[X] = \frac{1}{10\sqrt{n}} \cdot n = \frac{\sqrt{n}}{10} $$
Then the Chernoff bound would give us $1 - \delta$ confidence interval of
\begin{align*}
  && \Pr[X \ge \E[X] + t] \le e^{-\frac{2t^2}{n}} = \frac{\delta}{2} \\
  &\Rightarrow &t = \sqrt{\frac{n\log(2/\delta)}{2}}
\end{align*}
So the Chernoff bound gives us $X \in \E[X] \pm \sqrt{\frac{n\log(2/\delta)}{2}}$ with probability $1 - \delta$.

If we pick $\delta = 0.10$, then the bound is $X \in \E[X] \pm \sqrt{10n}$. Note that the width of our confidence interval is about 60 times the expected value!

We know that the bound can be made much tighter here because the variance of the $X_i$ is small. The Chernoff bound doesn't take variance into account. Later in this class, we will encounter Bernstein-type inequalities, which do account for the variance.

Also, note that the Chernoff bound can be less than 0 or greater than $n$, which does not make sense in this problem.

\subsection{Second Food for Thought}
\textbf{Suppose you have a biased coin that lands heads with an unknown probability $p$. How many flips will it take to learn $p$ to an error of $\pm \eps$ with probability $1 - \delta$?}

We need to find $n$ such that
$$ \Pr[X \ge (p + \eps)n] \le \frac{\delta}{2} $$
By the Chernoff bound,
\begin{align*}
  \Pr[X \ge (p + \eps)n] &\le e^{-\frac{2(\eps n)^2}{n}} \\
                         &= e^{-2\eps^2 n}
\end{align*}
So we must pick $n$ such that
$$ e^{-2\eps^2 n} \le \frac \delta 2 $$
i.e.
$$ n \ge \frac{1}{2\eps^2}\log\left(\frac{2}{\delta} \right) $$

\section{The Coupon Collecting Problem}

\textbf{Suppose there exist $n$ types of coupons, with an infinite supply of each. If you collect coupons uniformly at random, what is the expected number of coupons you must collect to hold at least one of each type?}

Let's consider the case in which we hold $i$ types of coupons already and determine how many we expect to collect before obtaining a new type; denote this expectation as $T_i$.

Then
\begin{align*}
  T_0 &= 1 \\
  T_1 &= 2 \\
  \ldots \\
  T_{n-1} &= n \\
  \ldots \\
  T_{n-k} &= n / k \\
  \Rightarrow T = \sum{T_i} &= n \sum_{i=1}^n \frac{1}{k} \leq n(\log n + 1)
\end{align*}
Note that $T_{n-k} = n / k$ precisely because we draw coupons uniformly at random.

\section{Background on Complexity Theory}

Let's begin our whirlwind tour of complexity theory. We'll explore, to varying degrees, \Pt,\ \NP,\ab\ \ZPP,\ \RP,\ \coRP, and \BPP.

\subsection{\Pt: Polynomial-time deterministic algorithms}

These can be described by Turing machines and $languages$. A $language$ $\ell$

\subsection{\NP: \Pt\ with a good ``advice'' string}

\subsection{\ZPP\: Zero-error with expected polynomial time}

\subsection{\RP\ and \coRP: One-sided error }

\subsection{\PP\ and \BPP: Two-sided error }

\subsection{\Ppoly\ and amplification}

\subsection{Containment}

\section{Adleman's Theorem}

\bibliographystyle{alpha}

\begin{thebibliography}{42}

% \bibitem[AMS99]{AMS99}
% Noga~Alon, Yossi~Matias, Mario~Szegedy.
% \newblock The Space Complexity of Approximating the Frequency Moments.
% \newblock {\em J. Comput. Syst. Sci.}, 58(1):137--147, 1999.

\end{thebibliography}

\end{document}
