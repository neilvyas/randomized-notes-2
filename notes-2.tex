\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}
\DeclareMathOperator*{\var}{\mathbf{Var}}

\newcommand{\eps}{\epsilon}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\R}{\mathbb{R}}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf CS 388R: Randomized Algorithms } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe: #4}{Lecture #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex

\begin{document}

\lecture{2 --- 31 August 2014}{Fall 2015}{Prof.\ Eric Price}{Sid Kapur, Neil Vyas}

\section{Overview}

In the last lecture we \ldots.

In this lecture we \ldots.

\section{Food for Thought}

\begin{enumerate}
  \item Suppose you flip an unbiased coin 1000 times. How surprised would you be if we observed
    \begin{itemize}
      \item 500 heads?
      \item 510 heads?
      \item 600 heads?
      \item 1000 heads?
    \end{itemize}
  \item Suppose you have a biased coin that lands heads with an unknown probability $p$. How many flips will it take to learn $p$ to an error of $\pm \eps$ with probability $1 - \delta$?
\end{enumerate}

\section{Probability Background}
\begin{theorem}
  \textbf{Linearity of expectation.}

  If $X_1,\dots,X_n$ are random variables, then
  $$ \E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \E[X_i] $$
\end{theorem}

Example: Suppose $X_1,\dots,X_n$ are Bernoulli trials with $p=0.5$. Let $X = \sum_{i=1}^n E_i$. Then
$$ \E[X] = \sum_{i=1}^n \E[X_i] = \frac{n}{2} $$

\begin{definition}
  The variance of a random variable $X$, denoted $\var[X]$, is defined as
  $$ \var[X] = \E\left[(X - \E[X])^2\right] $$
\end{definition}

\begin{observation}
Note that if $Y_1$ and $Y_2$ are independent random variables with $\E[Y_1] = \E[Y_2] = 0$, then
\begin{align*}
  \var[Y_1 + Y_2] &= \E\left[(Y_1 + Y_2)^2\right] \\
                  &= \E\left[Y_1^2 + Y_2^2 + 2Y_1Y_2\right] \\
                  &= \E\left[Y_1^2\right] + \E\left[Y_2^2\right] + 2\E[Y_1]\E[Y_2] &\small\text{(since $Y_1$ and $Y_2$ are independent)}\\
                  &= \E\left[Y_1^2\right] + \E\left[Y_2^2\right] \\
                  &= \var[Y_1] + \var[Y_2]
\end{align*}
\end{observation}

\begin{claim}\label{claima}
We claim (without proof) that if $Y_1$ and $Y_2$ are independent, then $\var[Y_1 + Y_2] = \var[Y_1] + \var[Y_2]$ (i.e. $\E[Y_1]$ and $\E[Y_2]$ need not be zero).
\end{claim}

Example: Suppose $X_1,\dots,X_n$ are independent Bernoulli trials with $p=0.5$. Let $X = \sum_{i=1}^n E_i$. Then
\begin{align*}
  \var[X] &= \sum_{i=1}^n \var[X_i] &\small\text{(by Claim \ref{claima})}\\
          &= n\left[ \frac12 \left(\frac12\right)^2 + \frac12 \left(\frac12\right)^2 \right] \\
          &= \frac{n}{4}
\end{align*}

If $n=1000$, then $\var[X] = 250$ and the standard deviation of $X = \sqrt{\var[X]} \approx 16$.

\begin{theorem}
  \textbf{The central limit theorem}
  Suppose $X_1,\dots,X_n$ are independent identically distributed random variables with expected value $\mu$ and variance $\sigma^2$.

  Then as $n \to \infty$, the sample average $S_n = \frac{X_1 + \dots + X_n}{n}$ converges to the Gaussian
  $$ N\left(\mu, \frac{\sigma^2}{n}\right) $$

  Some statements of the central limit theorem give bounds on convergence, but unfortunately these bounds are not tight enough to be useful in this example.
\end{theorem}

By the central limit theorem, as the number of coin tosses goes to infinity, our binomial distribution of coin tosses converges to a Gaussian $N(\mu, \sigma^2)$ where $\mu = \frac{n}{2}$ and $\sigma^2 = \frac{n}{4}$, as we calculated above.

\section{Bounding the Sum of Bernoulli Trials}

Let's return to our coin-tossing problem.

Can we find an upper bound on the probability that the number of coin tosses exceeds a certain number?

\begin{claim}\label{gaussianbound}
  If $Z$ is drawn from a Gaussian with mean $\mu$ and variance $\sigma^2$, then
  \begin{align*}
    \Pr[Z \ge \mu + t] &\le e^{-\frac{t^2}{2\sigma^2}} \quad \forall t \ge 0 \\
    \Pr[Z \le \mu - t] &\le e^{-\frac{t^2}{2\sigma^2}} \quad \forall t \ge 0
  \end{align*}

\end{claim}
If our distribution were Gaussian, we could use the bound from Claim \ref{gaussianbound}:
\begin{align*}
  &&\Pr[X \ge 500 + \sigma^2 t] &\le e^{-\frac{t^2}{2}} \\
  &\Rightarrow &\Pr[X \ge 500 + 16t] &\le e^{-\frac{t^2}{2}} \\
  &\Rightarrow &\Pr[X \ge 600] &\le e^{-18} \\
\end{align*}



\section{The Coupon Collecting Problem}

\section{Background on Complexity Theory}

\section{Adleman's Theorem}

\bibliographystyle{alpha}

\begin{thebibliography}{42}

% \bibitem[AMS99]{AMS99}
% Noga~Alon, Yossi~Matias, Mario~Szegedy.
% \newblock The Space Complexity of Approximating the Frequency Moments.
% \newblock {\em J. Comput. Syst. Sci.}, 58(1):137--147, 1999.

\end{thebibliography}

\end{document}
